from rest_framework import generics, permissions
from rest_framework.decorators import api_view
from rest_framework.response import Response
from rest_framework import status
from .models import PerfilRedeSocial, Postagem, AlertaMonitoramento
from .serializers import PerfilRedeSocialSerializer, PostagemSerializer, AlertaMonitoramentoSerializer
from .scraper import SocialMediaScraperService
from django.db.models import Prefetch
import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime
import time
from threading import Thread


class PerfilRedeSocialListCreateView(generics.ListCreateAPIView):
    queryset = PerfilRedeSocial.objects.all()
    serializer_class = PerfilRedeSocialSerializer
    permission_classes = [permissions.IsAuthenticated]

    def perform_create(self, serializer):
        serializer.save(monitorado_por=self.request.user)


class PerfilRedeSocialRetrieveUpdateDestroyView(generics.RetrieveUpdateDestroyAPIView):
    queryset = PerfilRedeSocial.objects.all()
    serializer_class = PerfilRedeSocialSerializer
    permission_classes = [permissions.IsAuthenticated]


class PostagemListCreateView(generics.ListCreateAPIView):
    queryset = Postagem.objects.all()
    serializer_class = PostagemSerializer
    permission_classes = [permissions.IsAuthenticated]


class PostagemRetrieveUpdateDestroyView(generics.RetrieveUpdateDestroyAPIView):
    queryset = Postagem.objects.all()
    serializer_class = PostagemSerializer
    permission_classes = [permissions.IsAuthenticated]


class AlertaMonitoramentoListCreateView(generics.ListCreateAPIView):
    queryset = AlertaMonitoramento.objects.all()
    serializer_class = AlertaMonitoramentoSerializer
    permission_classes = [permissions.IsAuthenticated]

    def perform_create(self, serializer):
        serializer.save(monitorado_por=self.request.user)


class AlertaMonitoramentoRetrieveUpdateDestroyView(generics.RetrieveUpdateDestroyAPIView):
    queryset = AlertaMonitoramento.objects.all()
    serializer_class = AlertaMonitoramentoSerializer
    permission_classes = [permissions.IsAuthenticated]


@api_view(['POST'])
def sync_profile_data(request, perfil_id):
    """Sync profile data from social media platform"""
    scraper_service = SocialMediaScraperService()
    success = scraper_service.sync_profile_data(perfil_id)
    
    if success:
        return Response({'message': 'Profile data synced successfully'}, status=status.HTTP_200_OK)
    else:
        return Response({'error': 'Failed to sync profile data'}, status=status.HTTP_400_BAD_REQUEST)


@api_view(['POST'])
def sync_profile_posts(request, perfil_id):
    """Sync posts from a social media profile"""
    limit = request.data.get('limit', 10)
    scraper_service = SocialMediaScraperService()
    new_posts_count = scraper_service.sync_profile_posts(perfil_id, limit)
    
    return Response({
        'message': f'Synced {new_posts_count} new posts',
        'new_posts_count': new_posts_count
    }, status=status.HTTP_200_OK)


@api_view(['POST'])
def scrape_profile_and_posts(request, perfil_id):
    """Complete sync of profile and posts"""
    limit = request.data.get('limit', 10)
    scraper_service = SocialMediaScraperService()
    result = scraper_service.scrape_profile_and_posts(perfil_id, limit)
    
    return Response(result, status=status.HTTP_200_OK)


@api_view(['GET'])
def scrape_platform_posts(request, plataforma):
    """Scrape multiple profiles from a specific platform"""
    limit = request.GET.get('limit', 10)
    try:
        limit = int(limit)
    except ValueError:
        limit = 10
    
    scraper_service = SocialMediaScraperService()
    results = scraper_service.scrape_by_platform(plataforma, limit)
    
    return Response(results, status=status.HTTP_200_OK)


@api_view(['POST'])
def scrape_generic_content(request):
    """Scrape generic social media content from a URL"""
    url = request.data.get('url')
    if not url:
        return Response({'error': 'URL is required'}, status=status.HTTP_400_BAD_REQUEST)
    
    scraper_service = SocialMediaScraperService()
    posts = scraper_service.scraper.scrape_generic_social_content(url)
    
    return Response({'posts': posts, 'count': len(posts)}, status=status.HTTP_200_OK)


@api_view(['GET'])
def get_atividades_monitorizadas(request):
    """Get all posts with related profile information for monitoring display"""
    try:
        # Get posts with related profile data
        posts = Postagem.objects.select_related('perfil').all()
        
        # Format the data to match the frontend's expectations
        atividades = []
        for post in posts:
            atividade = {
                'id': post.id,
                'perfil': post.perfil.id,
                'post_id': post.post_id,
                'conteudo': post.conteudo,
                'data_postagem': post.data_postagem.isoformat() if post.data_postagem else None,
                'curtidas': post.curtidas,
                'compartilhamentos': post.compartilhamentos,
                'comentarios': post.comentarios,
                'url_postagem': post.url_postagem,
                'marcadores': post.marcadores,
                'relevancia': post.relevancia,
                'data_coleta': post.data_coleta.isoformat() if post.data_coleta else None,
                'plataforma': post.perfil.plataforma,
                'nome_usuario': post.perfil.nome_usuario,
            }
            atividades.append(atividade)
        
        return Response(atividades, status=status.HTTP_200_OK)
    except Exception as e:
        return Response({'error': str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


@api_view(['GET'])
def get_google_alerts_rss(request):
    """Get Google Alerts RSS feed data similar to the example implementation"""
    try:
        # In a real implementation, this would fetch from configured Google Alert RSS feeds
        # For now, we'll simulate the functionality based on the main.py example
        
        # This would typically come from a model that stores Google Alert configurations
        # For demo purposes, I'll create a function that mimics the main.py functionality
        def get_clean_url(google_proxy_url: str) -> str:
            """Remove Google redirect and get the actual URL"""
            match = re.search(r'[?&]url=([^&]+)', google_proxy_url)
            if match:
                import urllib.parse
                return urllib.parse.unquote(match.group(1))
            
            # Fallback: try to follow redirect (may fail in some cases)
            try:
                response = requests.get(google_proxy_url, timeout=8, allow_redirects=True)
                return response.url
            except Exception:
                return google_proxy_url
        
        # Sample Google Alert RSS feed URL (would be configurable in real implementation)
        # For demonstration, I'll create mock data similar to main.py
        sample_alerts = []
        
        # In a real implementation, we would fetch from actual Google Alerts RSS feed like:
        # response = requests.get(RSS_FEED_URL, headers=headers, timeout=12)
        # soup = BeautifulSoup(response.text, "xml")
        # entries = soup.find_all("entry")
        # 
        # For now, return sample data that follows the same structure as main.py
        sample_alerts = [
            {
                "title": "Pessoa de Interesse mencionada em notícia",
                "published": "2025-12-24T10:30:00Z",
                "url": "https://noticiaexemplo.com/pessoa-interesse",
                "content": "Conteúdo do alerta do Google com menções relevantes a pessoas ou termos de interesse...",
                "source": "Exemplo de Fonte de Notícia"
            },
            {
                "title": "Menção em redes sociais sobre tópico monitorado",
                "published": "2025-12-24T09:15:00Z", 
                "url": "https://redeexemplo.com/mencao",
                "content": "Outra menção relevante detectada por meio do sistema de alertas...",
                "source": "Exemplo de Rede Social"
            }
        ]
        
        return Response(sample_alerts, status=status.HTTP_200_OK)
    except Exception as e:
        return Response({'error': str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


@api_view(['POST'])
def add_google_alert(request):
    """Add a new Google Alert RSS feed to monitor"""
    try:
        # Extract data from request
        feed_url = request.data.get('feed_url')
        name = request.data.get('name', 'Google Alert')
        
        if not feed_url:
            return Response({'error': 'Feed URL is required'}, status=status.HTTP_400_BAD_REQUEST)
        
        # In a real implementation, we would validate the RSS feed and store it
        # For now, return success
        return Response({
            'message': 'Google Alert added successfully',
            'name': name,
            'feed_url': feed_url
        }, status=status.HTTP_201_CREATED)
    except Exception as e:
        return Response({'error': str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


@api_view(['POST'])
def criar_alerta_monitoramento(request):
    """Create a new monitoring alert for a search term"""
    if not request.user.is_authenticated:
        return Response({'error': 'Usuário não autenticado'}, status=status.HTTP_401_UNAUTHORIZED)
        
    try:
        termo_busca = request.data.get('termo_busca')
        descricao = request.data.get('descricao', '')
        
        if not termo_busca:
            return Response({'error': 'Termo de busca é obrigatório'}, status=status.HTTP_400_BAD_REQUEST)
        
        # Create the alert
        alerta = AlertaMonitoramento.objects.create(
            termo_busca=termo_busca,
            descricao=descricao,
            ativo=True,
            monitorado_por=request.user
        )
        
        serializer = AlertaMonitoramentoSerializer(alerta)
        return Response(serializer.data, status=status.HTTP_201_CREATED)
    except Exception as e:
        return Response({'error': str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


@api_view(['GET'])
def buscar_resultados_alerta(request, alerta_id):
    """Search for results related to a specific alert"""
    try:
        alerta = AlertaMonitoramento.objects.get(id=alerta_id)
        
        # In a real implementation, this would search the internet for the term
        # For now, we'll return mock results
        # In practice, this would use search engines APIs or web scraping
        
        # Update the last checked time
        alerta.data_ultima_verificacao = datetime.now()
        alerta.save()
        
        # Mock search results
        resultados = [
            {
                "titulo": f"Resultado para '{alerta.termo_busca}' - Notícia Exemplo",
                "descricao": f"Esta é uma notícia que menciona o termo '{alerta.termo_busca}' em um contexto relevante.",
                "url": "https://exemplo.com/noticia",
                "fonte": "Exemplo de Fonte",
                "data": datetime.now().isoformat()
            }
        ]
        
        return Response(resultados, status=status.HTTP_200_OK)
    except AlertaMonitoramento.DoesNotExist:
        return Response({'error': 'Alerta não encontrado'}, status=status.HTTP_404_NOT_FOUND)
    except Exception as e:
        return Response({'error': str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


@api_view(['GET'])
def buscar_todos_alertas_resultados(request):
    """Search for results for all active alerts"""
    try:
        alertas = AlertaMonitoramento.objects.filter(ativo=True)
        resultados_completos = []
        
        for alerta in alertas:
            # In a real implementation, this would search the internet for the term
            # For now, we'll return mock results
            resultados = [
                {
                    "alerta_id": alerta.id,
                    "termo_busca": alerta.termo_busca,
                    "resultados": [
                        {
                            "titulo": f"Resultado para '{alerta.termo_busca}'",
                            "descricao": f"Menção encontrada para o termo '{alerta.termo_busca}'",
                            "url": "https://exemplo.com/mencao",
                            "fonte": "Fonte Exemplo",
                            "data": datetime.now().isoformat()
                        }
                    ]
                }
            ]
            resultados_completos.extend(resultados)
        
        # Update the last checked time for all alerts
        alertas.update(data_ultima_verificacao=datetime.now())
        
        return Response(resultados_completos, status=status.HTTP_200_OK)
    except Exception as e:
        return Response({'error': str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)